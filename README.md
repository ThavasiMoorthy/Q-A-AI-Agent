AI-Powered RAG Agent using LangGraph + Llama 3.1

A fully local Retrieval-Augmented Generation (RAG) pipeline built using LangGraph, Ollama (Llama 3.1), and Streamlit, enhanced with TruLens trace logging and automatic evaluation (BLEU, ROUGE, and BERTScore).

ğŸ“˜ Project Overview

This project demonstrates a Retrieval-Augmented Generation (RAG) workflow for answering questions based on the content of a large document (e.g., a Renewable Energy PDF).

It leverages:

ğŸ§© LangGraph â€“ to define a multi-step reasoning pipeline (plan â†’ retrieve â†’ answer â†’ reflect).

ğŸ¦™ Llama 3.1 (via Ollama) â€“ as the local LLM for answer generation and reflection.

ğŸ“š Chroma Vector Database â€“ to store and retrieve text embeddings.

ğŸ” Hugging Face Sentence Transformers â€“ for semantic embeddings.

ğŸ“Š TruLens â€“ for trace logging, explainability, and quality evaluation.

ğŸŒ Streamlit UI â€“ for an interactive Q&A interface.


User Query
   â”‚
   â–¼
[Plan Node] â”€ Decide whether to retrieve context
   â”‚
   â–¼
[Retrieve Node] â”€ Get relevant document chunks via Chroma DB
   â”‚
   â–¼
[Answer Node] â”€ Use Llama 3.1 to generate context-grounded answer
   â”‚
   â–¼
[Reflect Node] â”€ Evaluate accuracy & relevance of answer
   â”‚
   â–¼
[TruLens] â”€ Log run data, feedback, and evaluation metrics

ğŸ§© Key Components
Component	Purpose
LangGraph	Defines a graph-based RAG pipeline with nodes & state transitions.
Llama 3.1 (Ollama)	Generates answers and performs self-reflection locally.
Chroma Vector Store	Stores embeddings of the document for retrieval.
HuggingFace Embeddings	Creates embeddings (all-MiniLM-L6-v2) for semantic search.
TruLens	Logs LLM calls, tracks metrics, and evaluates quality of RAG responses.
BLEU / ROUGE / BERTScore	Evaluates similarity between generated and reference answers.
Streamlit	Provides an interactive UI for querying the model.

ğŸ§° Installation & Setup
1ï¸âƒ£ Clone the Repository
git clone https://github.com/yourusername/langgraph-llama-rag.git
cd langgraph-llama-rag

2ï¸âƒ£ Create and Activate Virtual Environment
python -m venv hackathon
hackathon\Scripts\activate        # Windows
# OR
source hackathon/bin/activate     # Linux/Mac

3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt


Example requirements.txt

langchain-community>=0.2.0
langgraph>=0.0.10
trulens-eval>=1.0.0
trulens-apps-langchain>=1.0.0
streamlit
chromadb
sentence-transformers
evaluate
ollama

4ï¸âƒ£ Ensure Ollama and Llama 3.1 Are Installed
ollama pull llama3.1

5ï¸âƒ£ Place Your PDF in the Project Folder

For example:

Renewable_Energy.pdf

â–¶ï¸ Run the App

Start the Streamlit interface:

streamlit run rag_agent_llama_streamlit.py


Then open your browser at:

http://localhost:8501

ğŸ’¡ Example Interaction

Question:

What is Nuclear Energy?

Generated Answer (by Llama 3.1):

Nuclear energy is the energy stored in the nucleus of an atom that holds the nucleus together. The nucleus of a uranium atom is an example.

Reflection:

Relevant â€” The answer provides a correct and concise definition aligned with the question.

Evaluation Scores:

Metric	Score
BLEU	0.92
ROUGE-L	0.88
BERTScore (F1)	0.95


ğŸ“Š TruLens Logging and Dashboard

TruLens automatically logs each query-answer pair.
To view the dashboard:

from trulens_eval import Tru
tru = Tru()
tru.run_dashboard()


Then open:

http://localhost:8501/trulens

ğŸ§± Folder Structure
â”œâ”€â”€ rag_agent_llama_streamlit.py   # Main app file
â”œâ”€â”€ Renewable_Energy.pdf           # Knowledge base
â”œâ”€â”€ requirements.txt               # Dependencies
â”œâ”€â”€ README.md                      # Documentation
â”œâ”€â”€ chroma_store_llama/            # Local vector store
â””â”€â”€ trulens_data/                  # Logs and traces
